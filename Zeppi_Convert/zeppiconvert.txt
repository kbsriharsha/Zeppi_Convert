import nltk
from nltk.tag.stanford import StanfordNERTagger

sentence = u"Twenty miles east of Reno, Nev., " \
    "where packs of wild mustangs roam free through " \
    "the parched landscape, Tesla Gigafactory 1 " \
    "sprawls near Interstate 80."

hostname -i
hostname -name

ls 

import os
print (os.getcwd())
os.environ['JAVA_HOME']

jar = '/nyl/data/home/t93kx7p/python/stanford-ner/stanford-ner/stanford-ner.jar'
model = '/nyl/data/home/t93kx7p/python/stanford-ner/stanford-ner/classifier/english.all.3class.distsim.crf.ser.gz'

ner_tagger = StanfordNERTagger(model, jar, encoding='utf8')

import sys
print (sys.version)
print (sc.version)

# Importing libraries
import pandas as pd
import re
import time
import numpy as np
import nltk
import collections
from nltk.corpus import stopwords

from pyspark import SQLContext
from pyspark.sql import HiveContext, Row
from pyspark.sql.functions import col
sqlContext = SQLContext(sc)

hc = HiveContext(sc)

ds = hc.sql("select * from source_orap03_be0101.beneficiary")

ds.count()

ds_req = ds.filter((ds.unformatted_tx.isNotNull()) | (ds.unformatted_tx != '')).select('unformatted_tx')

ds_req.count()

ds_list = ds_req.rdd.map(lambda x: x['unformatted_tx']).take(400000)

s = ds_list.str.cat(sep = " ")

str = " ".join(ds_list)

str

